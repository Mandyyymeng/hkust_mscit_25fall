{
  "california_schools": {
    "predicted_sql": {
      "count": 4700,
      "mean_time": 0.008145537477858523,
      "std_time": 0.006182394050418295,
      "min_time": 0.0001289844512939453,
      "max_time": 0.04280734062194824
    },
    "ground_truth_sql": {
      "count": 4700,
      "mean_time": 0.007196655374892214,
      "std_time": 0.0057594383352982375,
      "min_time": 0.0001239776611328125,
      "max_time": 0.0583803653717041
    }
  },
  "financial": {
    "predicted_sql": {
      "count": 7000,
      "mean_time": 0.031310922486441475,
      "std_time": 0.11439467150777129,
      "min_time": 7.271766662597656e-05,
      "max_time": 0.6725060939788818
    },
    "ground_truth_sql": {
      "count": 7000,
      "mean_time": 0.03145234775543213,
      "std_time": 0.1155455489819067,
      "min_time": 7.271766662597656e-05,
      "max_time": 0.6942107677459717
    }
  },
  "toxicology": {
    "predicted_sql": {
      "count": 9000,
      "mean_time": 0.0011852756341298421,
      "std_time": 0.0021572500116778556,
      "min_time": 5.125999450683594e-05,
      "max_time": 0.026456594467163086
    },
    "ground_truth_sql": {
      "count": 9000,
      "mean_time": 0.0008688522179921468,
      "std_time": 0.0021370890754940338,
      "min_time": 5.1975250244140625e-05,
      "max_time": 0.11764764785766602
    }
  },
  "card_games": {
    "predicted_sql": {
      "count": 11500,
      "mean_time": 0.05300154520117718,
      "std_time": 0.08954502453286473,
      "min_time": 0.00011920928955078125,
      "max_time": 0.6023328304290771
    },
    "ground_truth_sql": {
      "count": 11500,
      "mean_time": 0.05004864856471186,
      "std_time": 0.08481726271655828,
      "min_time": 0.00012564659118652344,
      "max_time": 0.6214282512664795
    }
  },
  "codebase_community": {
    "predicted_sql": {
      "count": 13500,
      "mean_time": 0.17771493747499253,
      "std_time": 1.248371108251367,
      "min_time": 0.00011396408081054688,
      "max_time": 25.7941153049469
    },
    "ground_truth_sql": {
      "count": 13500,
      "mean_time": 0.06989595596878617,
      "std_time": 0.11037632369456171,
      "min_time": 0.00011372566223144531,
      "max_time": 0.6699478626251221
    }
  },
  "superhero": {
    "predicted_sql": {
      "count": 11300,
      "mean_time": 0.00022630750605490355,
      "std_time": 0.0002642137098176893,
      "min_time": 6.890296936035156e-05,
      "max_time": 0.002452850341796875
    },
    "ground_truth_sql": {
      "count": 11300,
      "mean_time": 0.00023657431644676005,
      "std_time": 0.0002974007559730484,
      "min_time": 7.009506225585938e-05,
      "max_time": 0.0023589134216308594
    }
  },
  "formula_1": {
    "predicted_sql": {
      "count": 9900,
      "mean_time": 0.002487490875552399,
      "std_time": 0.007303001389281415,
      "min_time": 0.0001304149627685547,
      "max_time": 0.04805445671081543
    },
    "ground_truth_sql": {
      "count": 9900,
      "mean_time": 0.011878037115540167,
      "std_time": 0.09648442595794059,
      "min_time": 0.00013017654418945312,
      "max_time": 1.047701120376587
    }
  },
  "thrombosis_prediction": {
    "predicted_sql": {
      "count": 9600,
      "mean_time": 0.0008776094516118368,
      "std_time": 0.0009563650260795245,
      "min_time": 7.62939453125e-05,
      "max_time": 0.012094259262084961
    },
    "ground_truth_sql": {
      "count": 9600,
      "mean_time": 0.0007322295010089874,
      "std_time": 0.0008380198963199743,
      "min_time": 7.653236389160156e-05,
      "max_time": 0.003757953643798828
    }
  },
  "student_club": {
    "predicted_sql": {
      "count": 12900,
      "mean_time": 0.00016454055327777716,
      "std_time": 0.00045909353092849056,
      "min_time": 7.081031799316406e-05,
      "max_time": 0.006870269775390625
    },
    "ground_truth_sql": {
      "count": 12900,
      "mean_time": 0.0001631936361623365,
      "std_time": 0.0004573605982521528,
      "min_time": 7.05718994140625e-05,
      "max_time": 0.0073871612548828125
    }
  },
  "debit_card_specializing": {
    "predicted_sql": {
      "count": 4000,
      "mean_time": 0.01533719152212143,
      "std_time": 0.0304333610892828,
      "min_time": 6.198883056640625e-05,
      "max_time": 0.13480210304260254
    },
    "ground_truth_sql": {
      "count": 4000,
      "mean_time": 0.015200893521308899,
      "std_time": 0.03076054431029946,
      "min_time": 6.580352783203125e-05,
      "max_time": 0.13526129722595215
    }
  }
}