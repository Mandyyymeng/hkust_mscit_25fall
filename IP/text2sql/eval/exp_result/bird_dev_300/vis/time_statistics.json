{
  "california_schools": {
    "predicted_sql": {
      "count": 1500,
      "mean_time": 0.004629857222239176,
      "std_time": 0.0029938080487498424,
      "min_time": 0.0001304149627685547,
      "max_time": 0.03399205207824707
    },
    "ground_truth_sql": {
      "count": 1500,
      "mean_time": 0.003981398105621338,
      "std_time": 0.0029116501997079824,
      "min_time": 0.00013136863708496094,
      "max_time": 0.015842676162719727
    }
  },
  "financial": {
    "predicted_sql": {
      "count": 1700,
      "mean_time": 0.000538481824538287,
      "std_time": 0.00038547201474387424,
      "min_time": 8.416175842285156e-05,
      "max_time": 0.0014982223510742188
    },
    "ground_truth_sql": {
      "count": 1700,
      "mean_time": 0.0006804321793948902,
      "std_time": 0.0005658701118736659,
      "min_time": 8.797645568847656e-05,
      "max_time": 0.010501623153686523
    }
  },
  "toxicology": {
    "predicted_sql": {
      "count": 2100,
      "mean_time": 0.000680940378279913,
      "std_time": 0.0008834230856527714,
      "min_time": 5.507469177246094e-05,
      "max_time": 0.006525278091430664
    },
    "ground_truth_sql": {
      "count": 2100,
      "mean_time": 0.0008664015361240932,
      "std_time": 0.0017526897752378164,
      "min_time": 5.936622619628906e-05,
      "max_time": 0.051393985748291016
    }
  },
  "card_games": {
    "predicted_sql": {
      "count": 2300,
      "mean_time": 0.04379524106564729,
      "std_time": 0.06870471488646072,
      "min_time": 0.0001418590545654297,
      "max_time": 0.37665343284606934
    },
    "ground_truth_sql": {
      "count": 2300,
      "mean_time": 0.04487272096716839,
      "std_time": 0.06144060439624807,
      "min_time": 0.00013780593872070312,
      "max_time": 0.31994032859802246
    }
  },
  "codebase_community": {
    "predicted_sql": {
      "count": 1600,
      "mean_time": 0.07358356520533561,
      "std_time": 0.0905740605808469,
      "min_time": 0.0001087188720703125,
      "max_time": 0.4632902145385742
    },
    "ground_truth_sql": {
      "count": 1600,
      "mean_time": 0.07532740280032157,
      "std_time": 0.09803737387517703,
      "min_time": 0.0001201629638671875,
      "max_time": 0.4220573902130127
    }
  },
  "superhero": {
    "predicted_sql": {
      "count": 2600,
      "mean_time": 0.0002561957102555495,
      "std_time": 0.00027484297259811866,
      "min_time": 7.128715515136719e-05,
      "max_time": 0.002368927001953125
    },
    "ground_truth_sql": {
      "count": 2600,
      "mean_time": 0.0002372711438399095,
      "std_time": 0.0002264548208973578,
      "min_time": 7.152557373046875e-05,
      "max_time": 0.0028963088989257812
    }
  },
  "formula_1": {
    "predicted_sql": {
      "count": 1600,
      "mean_time": 0.004645007997751236,
      "std_time": 0.010488261579837113,
      "min_time": 0.00013184547424316406,
      "max_time": 0.05286836624145508
    },
    "ground_truth_sql": {
      "count": 1600,
      "mean_time": 0.06392630860209465,
      "std_time": 0.22991494031705415,
      "min_time": 0.0001316070556640625,
      "max_time": 1.0232641696929932
    }
  },
  "thrombosis_prediction": {
    "predicted_sql": {
      "count": 1700,
      "mean_time": 0.0007213620578541475,
      "std_time": 0.000987665840301257,
      "min_time": 8.058547973632812e-05,
      "max_time": 0.0052776336669921875
    },
    "ground_truth_sql": {
      "count": 1700,
      "mean_time": 0.0008059876105364632,
      "std_time": 0.001155951933463353,
      "min_time": 7.963180541992188e-05,
      "max_time": 0.013762235641479492
    }
  },
  "student_club": {
    "predicted_sql": {
      "count": 2200,
      "mean_time": 0.00026114962317726827,
      "std_time": 0.000664444309101596,
      "min_time": 7.081031799316406e-05,
      "max_time": 0.004400491714477539
    },
    "ground_truth_sql": {
      "count": 2200,
      "mean_time": 0.00027228344570506705,
      "std_time": 0.0006563435896081774,
      "min_time": 7.009506225585938e-05,
      "max_time": 0.004216909408569336
    }
  },
  "debit_card_specializing": {
    "predicted_sql": {
      "count": 1200,
      "mean_time": 0.02898738920688629,
      "std_time": 0.04628788770731685,
      "min_time": 6.246566772460938e-05,
      "max_time": 0.16530299186706543
    },
    "ground_truth_sql": {
      "count": 1200,
      "mean_time": 0.014257972240447997,
      "std_time": 0.021145317643033044,
      "min_time": 7.128715515136719e-05,
      "max_time": 0.07183289527893066
    }
  }
}