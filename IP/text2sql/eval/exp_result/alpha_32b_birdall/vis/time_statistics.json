{
  "california_schools": {
    "predicted_sql": {
      "count": 5500,
      "mean_time": 0.005061391613700173,
      "std_time": 0.003678513514498699,
      "min_time": 9.393692016601562e-05,
      "max_time": 0.05418682098388672
    },
    "ground_truth_sql": {
      "count": 5500,
      "mean_time": 0.005445735281163996,
      "std_time": 0.004147822240516118,
      "min_time": 0.00010228157043457031,
      "max_time": 0.02800917625427246
    }
  },
  "financial": {
    "predicted_sql": {
      "count": 7000,
      "mean_time": 0.018697337763650075,
      "std_time": 0.09972920789308429,
      "min_time": 7.414817810058594e-05,
      "max_time": 0.9596428871154785
    },
    "ground_truth_sql": {
      "count": 7000,
      "mean_time": 0.022601375954491753,
      "std_time": 0.10821994246161874,
      "min_time": 7.462501525878906e-05,
      "max_time": 1.0925872325897217
    }
  },
  "toxicology": {
    "predicted_sql": {
      "count": 10000,
      "mean_time": 0.001418960213661194,
      "std_time": 0.00306164937180606,
      "min_time": 5.078315734863281e-05,
      "max_time": 0.03547549247741699
    },
    "ground_truth_sql": {
      "count": 10000,
      "mean_time": 0.0017302349090576171,
      "std_time": 0.007674478255462115,
      "min_time": 4.982948303222656e-05,
      "max_time": 0.1039879322052002
    }
  },
  "card_games": {
    "predicted_sql": {
      "count": 11600,
      "mean_time": 0.035009346933200444,
      "std_time": 0.06061274236530899,
      "min_time": 0.00011420249938964844,
      "max_time": 0.4444310665130615
    },
    "ground_truth_sql": {
      "count": 11600,
      "mean_time": 0.03679524429913225,
      "std_time": 0.060045630504095726,
      "min_time": 0.00011134147644042969,
      "max_time": 0.43810558319091797
    }
  },
  "codebase_community": {
    "predicted_sql": {
      "count": 13400,
      "mean_time": 0.037020677523826484,
      "std_time": 0.05965244698922316,
      "min_time": 9.489059448242188e-05,
      "max_time": 0.5109245777130127
    },
    "ground_truth_sql": {
      "count": 13400,
      "mean_time": 0.03786195125152816,
      "std_time": 0.05998834294125601,
      "min_time": 9.608268737792969e-05,
      "max_time": 0.403583288192749
    }
  },
  "superhero": {
    "predicted_sql": {
      "count": 11300,
      "mean_time": 0.00030812537775630445,
      "std_time": 0.0005196410706458976,
      "min_time": 7.200241088867188e-05,
      "max_time": 0.006171464920043945
    },
    "ground_truth_sql": {
      "count": 11300,
      "mean_time": 0.00026826609552434063,
      "std_time": 0.0003766843118861055,
      "min_time": 7.176399230957031e-05,
      "max_time": 0.005351066589355469
    }
  },
  "formula_1": {
    "predicted_sql": {
      "count": 11100,
      "mean_time": 0.003103808476044251,
      "std_time": 0.00874647614057082,
      "min_time": 0.00013017654418945312,
      "max_time": 0.07427334785461426
    },
    "ground_truth_sql": {
      "count": 11100,
      "mean_time": 0.012237815148121603,
      "std_time": 0.10314539674564542,
      "min_time": 0.0001308917999267578,
      "max_time": 1.8375704288482666
    }
  },
  "thrombosis_prediction": {
    "predicted_sql": {
      "count": 8900,
      "mean_time": 0.0008155904994921738,
      "std_time": 0.0010513453385180204,
      "min_time": 7.82012939453125e-05,
      "max_time": 0.007727622985839844
    },
    "ground_truth_sql": {
      "count": 8900,
      "mean_time": 0.0007523498374424624,
      "std_time": 0.0009739057513592381,
      "min_time": 7.939338684082031e-05,
      "max_time": 0.017396926879882812
    }
  },
  "student_club": {
    "predicted_sql": {
      "count": 13800,
      "mean_time": 0.0002162474134693975,
      "std_time": 0.0005488771846133114,
      "min_time": 7.295608520507812e-05,
      "max_time": 0.007447242736816406
    },
    "ground_truth_sql": {
      "count": 13800,
      "mean_time": 0.00021154465882674506,
      "std_time": 0.0005421949796921886,
      "min_time": 7.414817810058594e-05,
      "max_time": 0.007293224334716797
    }
  },
  "debit_card_specializing": {
    "predicted_sql": {
      "count": 4200,
      "mean_time": 0.034367100795110066,
      "std_time": 0.0610740341056795,
      "min_time": 5.936622619628906e-05,
      "max_time": 0.3143470287322998
    },
    "ground_truth_sql": {
      "count": 4200,
      "mean_time": 0.02404321613765898,
      "std_time": 0.045935899320356514,
      "min_time": 6.175041198730469e-05,
      "max_time": 0.33922266960144043
    }
  }
}