{
  "california_schools": {
    "predicted_sql": {
      "count": 4900,
      "mean_time": 0.005626001504002785,
      "std_time": 0.0042420503109479235,
      "min_time": 9.560585021972656e-05,
      "max_time": 0.05328202247619629
    },
    "ground_truth_sql": {
      "count": 4900,
      "mean_time": 0.0049762253858605205,
      "std_time": 0.004046616415398093,
      "min_time": 9.751319885253906e-05,
      "max_time": 0.019571542739868164
    }
  },
  "financial": {
    "predicted_sql": {
      "count": 5900,
      "mean_time": 0.026950283010127182,
      "std_time": 0.1061561656124393,
      "min_time": 7.271766662597656e-05,
      "max_time": 0.7576987743377686
    },
    "ground_truth_sql": {
      "count": 5900,
      "mean_time": 0.029598507719524835,
      "std_time": 0.11263554434627232,
      "min_time": 7.319450378417969e-05,
      "max_time": 0.6902155876159668
    }
  },
  "toxicology": {
    "predicted_sql": {
      "count": 8300,
      "mean_time": 0.0010321599029632937,
      "std_time": 0.0016839829709075307,
      "min_time": 5.030632019042969e-05,
      "max_time": 0.018712520599365234
    },
    "ground_truth_sql": {
      "count": 8300,
      "mean_time": 0.0009880411194031497,
      "std_time": 0.0016727806191122593,
      "min_time": 5.078315734863281e-05,
      "max_time": 0.014142036437988281
    }
  },
  "card_games": {
    "predicted_sql": {
      "count": 11400,
      "mean_time": 0.05385716850297493,
      "std_time": 0.09180020444262287,
      "min_time": 0.00011467933654785156,
      "max_time": 0.7067265510559082
    },
    "ground_truth_sql": {
      "count": 11400,
      "mean_time": 0.057226546438116775,
      "std_time": 0.09180193148050401,
      "min_time": 0.000118255615234375,
      "max_time": 0.7149784564971924
    }
  },
  "codebase_community": {
    "predicted_sql": {
      "count": 13500,
      "mean_time": 0.05354895355083324,
      "std_time": 0.09012059787705755,
      "min_time": 9.512901306152344e-05,
      "max_time": 0.5682008266448975
    },
    "ground_truth_sql": {
      "count": 13500,
      "mean_time": 0.05228219171806618,
      "std_time": 0.09294943848635995,
      "min_time": 9.489059448242188e-05,
      "max_time": 0.6433272361755371
    }
  },
  "superhero": {
    "predicted_sql": {
      "count": 11400,
      "mean_time": 0.0002642947330809476,
      "std_time": 0.0003328634397699884,
      "min_time": 7.224082946777344e-05,
      "max_time": 0.0037958621978759766
    },
    "ground_truth_sql": {
      "count": 11400,
      "mean_time": 0.00025276966262281986,
      "std_time": 0.0003029424477452742,
      "min_time": 7.200241088867188e-05,
      "max_time": 0.0023267269134521484
    }
  },
  "formula_1": {
    "predicted_sql": {
      "count": 10200,
      "mean_time": 0.002839895112841737,
      "std_time": 0.007655998157871809,
      "min_time": 0.00012969970703125,
      "max_time": 0.08322715759277344
    },
    "ground_truth_sql": {
      "count": 10200,
      "mean_time": 0.002724151517830643,
      "std_time": 0.007870242699448924,
      "min_time": 0.00012922286987304688,
      "max_time": 0.06461501121520996
    }
  },
  "european_football_2": {
    "predicted_sql": {
      "count": 9000,
      "mean_time": 0.056529544803831315,
      "std_time": 0.09780790323039353,
      "min_time": 0.0001659393310546875,
      "max_time": 0.9982943534851074
    },
    "ground_truth_sql": {
      "count": 9000,
      "mean_time": 0.04875561708874172,
      "std_time": 0.05881053347964226,
      "min_time": 0.0001723766326904297,
      "max_time": 0.3677349090576172
    }
  },
  "thrombosis_prediction": {
    "predicted_sql": {
      "count": 10200,
      "mean_time": 0.0008021317276300169,
      "std_time": 0.0009016323005657346,
      "min_time": 7.677078247070312e-05,
      "max_time": 0.0040776729583740234
    },
    "ground_truth_sql": {
      "count": 10200,
      "mean_time": 0.0007587861781026803,
      "std_time": 0.0008352310157031719,
      "min_time": 7.62939453125e-05,
      "max_time": 0.005219459533691406
    }
  },
  "student_club": {
    "predicted_sql": {
      "count": 12900,
      "mean_time": 0.00016003420186597248,
      "std_time": 0.00041444891473646435,
      "min_time": 7.2479248046875e-05,
      "max_time": 0.00391387939453125
    },
    "ground_truth_sql": {
      "count": 12900,
      "mean_time": 0.0001614061436911886,
      "std_time": 0.0004317620429647661,
      "min_time": 7.271766662597656e-05,
      "max_time": 0.004443645477294922
    }
  },
  "debit_card_specializing": {
    "predicted_sql": {
      "count": 4000,
      "mean_time": 0.021118309199810028,
      "std_time": 0.04163124011760086,
      "min_time": 6.341934204101562e-05,
      "max_time": 0.20550251007080078
    },
    "ground_truth_sql": {
      "count": 4000,
      "mean_time": 0.015706153094768523,
      "std_time": 0.03619934963490742,
      "min_time": 6.818771362304688e-05,
      "max_time": 0.16725444793701172
    }
  }
}