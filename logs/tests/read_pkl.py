import pickle
import glob
import sys
from datetime import datetime
from pathlib import Path

class FileLogger:
    """æ–‡ä»¶æ—¥å¿—è¾“å‡ºç±»"""
    
    def __init__(self, log_file):
        self.log_file = log_file
        self.original_stdout = sys.stdout
        
    def write(self, text):
        """åªè¾“å‡ºåˆ°æ–‡ä»¶ï¼Œä¸è¾“å‡ºåˆ°æ§åˆ¶å°"""
        if self.log_file:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(text)
    
    def flush(self):
        if self.log_file:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.flush()

def setup_logging(output_file):
    """è®¾ç½®æ—¥å¿—è¾“å‡ºåˆ°æ–‡ä»¶"""
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    
    # æ¸…ç©ºæˆ–åˆ›å»ºæ–‡ä»¶å¹¶å†™å…¥å¤´éƒ¨
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"=== MCTS èŠ‚ç‚¹åˆ†ææ—¥å¿— ===\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 50 + "\n\n")
    
    # é‡å®šå‘æ ‡å‡†è¾“å‡ºåˆ°æ–‡ä»¶
    sys.stdout = FileLogger(output_file)

def print_node_details(node, is_root=False):
    """
    ç®€æ´åœ°æ‰“å°èŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯
    """
    if is_root:
        # æ ¹èŠ‚ç‚¹æ˜¾ç¤ºé€šç”¨ä¿¡æ¯
        print("  ğŸ“‹ æ ¹èŠ‚ç‚¹ä¿¡æ¯:")
        print(f"    ç±»å‹: {node.node_type.value}")
        print(f"    æ·±åº¦: {node.depth}")
        
        if hasattr(node, 'original_question') and node.original_question:
            question_preview = node.original_question[:80] + "..." if len(node.original_question) > 80 else node.original_question
            print(f"    é—®é¢˜: {question_preview}")
        
        if hasattr(node, 'hint') and node.hint:
            hint_preview = node.hint[:80] + "..." if len(node.hint) > 80 else node.hint
            print(f"    æç¤º: {hint_preview}")
        
        if hasattr(node, 'db_id'):
            print(f"    æ•°æ®åº“: {node.db_id}")
        
        if hasattr(node, 'schema_context') and node.schema_context:
            schema_preview = node.schema_context[:100] + "..." if len(node.schema_context) > 100 else node.schema_context
            print(f"    æ¨¡å¼ä¸Šä¸‹æ–‡é¢„è§ˆ: {schema_preview}")
        
        print("    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    else:
        # éæ ¹èŠ‚ç‚¹æ˜¾ç¤ºå…·ä½“æ“ä½œå†…å®¹
        node_type = node.node_type.value if hasattr(node.node_type, 'value') else node.node_type
        print(f"  ğŸ“‹ {node_type} èŠ‚ç‚¹ (æ·±åº¦: {node.depth}):")
        
        # æ˜¾ç¤ºçˆ¶åŠ¨ä½œç±»å‹
        if hasattr(node, 'parent_action') and node.parent_action:
            action_type = type(node.parent_action).__name__
            print(f"    çˆ¶åŠ¨ä½œ: {action_type}")
        
        # æ ¹æ®èŠ‚ç‚¹ç±»å‹å’Œå±æ€§æ˜¾ç¤ºå…·ä½“å†…å®¹
        if hasattr(node, 'rephrased_question') and node.rephrased_question:
            rephrased_preview = node.rephrased_question[:80] + "..." if len(node.rephrased_question) > 80 else node.rephrased_question
            print(f"    é‡è¿°é—®é¢˜: {rephrased_preview}")
        
        if hasattr(node, 'selected_schema_context') and node.selected_schema_context:
            selected_schema_preview = node.selected_schema_context
            print(f"    é€‰æ‹©æ¨¡å¼: {selected_schema_preview}")
        
        if hasattr(node, 'identified_column_values') and node.identified_column_values:
            column_values_preview = node.identified_column_values
            print(f"    è¯†åˆ«åˆ—å€¼: {column_values_preview}")
        
        if hasattr(node, 'identified_column_functions') and node.identified_column_functions:
            functions_preview = node.identified_column_functions
            print(f"    è¯†åˆ«å‡½æ•°: {functions_preview}")
        
        # SQLç›¸å…³
        if hasattr(node, 'sql_query') and node.sql_query:
            sql_preview = node.sql_query[:80] + "..." if len(node.sql_query) > 80 else node.sql_query
            print(f"    SQLæŸ¥è¯¢: {sql_preview}")
        
        if hasattr(node, 'revised_sql_query') and node.revised_sql_query:
            revised_preview = node.revised_sql_query[:80] + "..." if len(node.revised_sql_query) > 80 else node.revised_sql_query
            print(f"    ä¿®è®¢SQL: {revised_preview}")
        
        if hasattr(node, 'final_sql_query') and node.final_sql_query:
            final_preview = node.final_sql_query[:80] + "..." if len(node.final_sql_query) > 80 else node.final_sql_query
            print(f"    æœ€ç»ˆSQL: {final_preview}")
        
        # éªŒè¯å’Œè¯„åˆ†
        if hasattr(node, 'is_valid_sql_query') and node.is_valid_sql_query is not None:
            validity = "æœ‰æ•ˆ" if node.is_valid_sql_query else "æ— æ•ˆ"
            print(f"    SQLæœ‰æ•ˆæ€§: {validity}")
        
        if hasattr(node, 'consistency_score') and node.consistency_score is not None:
            print(f"    ä¸€è‡´æ€§è¯„åˆ†: {node.consistency_score:.3f}")
        
        # é€‰æ‹©çš„æ¨¡å¼ä¿¡æ¯
        if hasattr(node, 'selected_schema_dict') and node.selected_schema_dict:
            table_count = len(node.selected_schema_dict)
            column_count = sum(len(table.columns) for table in node.selected_schema_dict.values() if hasattr(table, 'columns'))
            print(f"    é€‰æ‹©æ¨¡å¼: {table_count}è¡¨/{column_count}åˆ—")
        
        # è·¯å¾„ä¿¡æ¯
        if hasattr(node, 'path_nodes') and node.path_nodes:
            print(f"    è·¯å¾„é•¿åº¦: {len(node.path_nodes)}")
        
        # ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(node, 'N') and hasattr(node, 'Q'):
            print(f"    è®¿é—®æ¬¡æ•°: {node.N}, ç´¯è®¡å¥–åŠ±: {node.Q:.2f}")
        
        print("    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

def print_simple_tree_structure(path, max_depth=5, show_details=False):
    """
    ç®€å•æ‰“å°å•ä¸ªè·¯å¾„çš„æ ‘å½¢ç»“æ„
    """
    if not path or len(path) == 0:
        print("  è·¯å¾„ä¸ºç©º")
        return
    
    print(f"\nğŸŒ³ è·¯å¾„æ ‘ç»“æ„ (é•¿åº¦: {len(path)} ä¸ªèŠ‚ç‚¹):")
    print("=" * 50)
    
    for i, node in enumerate(path):
        level = i
        prefix = "  " * level + "â””â”€â”€ " if i > 0 else "ğŸŒ± "
        
        # è·å–èŠ‚ç‚¹åŸºæœ¬ä¿¡æ¯
        node_type = getattr(node, 'node_type', 'Unknown')
        if hasattr(node_type, 'value'):
            node_type = node_type.value
        
        depth = getattr(node, 'depth', i)
        
        # æ˜¾ç¤ºèŠ‚ç‚¹ä¿¡æ¯
        node_info = f"{node_type} (æ·±åº¦: {depth})"
        
        # å¦‚æœæœ‰SQLæŸ¥è¯¢ï¼Œæ˜¾ç¤ºé¢„è§ˆ
        if hasattr(node, 'final_sql_query') and node.final_sql_query and i == len(path) - 1:
            sql_preview = node.final_sql_query[:50] + "..." if len(node.final_sql_query) > 50 else node.final_sql_query
            node_info += f" -> SQL: {sql_preview}"
        
        print(f"{prefix}{node_info}")
        
        # å¦‚æœéœ€è¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        if show_details:
            if i == 0:
                # æ ¹èŠ‚ç‚¹æ˜¾ç¤ºé€šç”¨ä¿¡æ¯
                print_node_details(node, is_root=True)
            else:
                # å…¶ä»–èŠ‚ç‚¹æ˜¾ç¤ºå…·ä½“æ“ä½œå†…å®¹
                print_node_details(node, is_root=False)
        
        # å¦‚æœè¾¾åˆ°æœ€å¤§æ·±åº¦ï¼Œåœæ­¢æ‰“å°
        if i >= max_depth - 1 and i < len(path) - 1:
            print("  " * (level + 1) + "â””â”€â”€ ... (åç»­èŠ‚ç‚¹çœç•¥)")
            break

def view_one_file(file_path, show_node_details=False):
    try:
        with open(file_path, 'rb') as f:
            data = pickle.load(f)
        
        print(f"  ç±»å‹: {type(data)}")
        print(f"  é•¿åº¦: {len(data) if hasattr(data, '__len__') else 'N/A'}")
        
        if isinstance(data, list):
            print(f"  åˆ—è¡¨åŒ…å« {len(data)} ä¸ªæ¨ç†è·¯å¾„")
            if len(data) > 0 and isinstance(data[0], list):
                avg_nodes = sum(len(path) for path in data) / len(data)
                print(f"  æ¯ä¸ªè·¯å¾„å¹³å‡èŠ‚ç‚¹æ•°: {avg_nodes:.1f}")
                
                # æ˜¾ç¤ºè·¯å¾„é•¿åº¦åˆ†å¸ƒ
                path_lengths = [len(path) for path in data]
                from collections import Counter
                length_counts = Counter(path_lengths)
                print(f"  è·¯å¾„é•¿åº¦åˆ†å¸ƒ: {dict(length_counts)}")
                
                # æ˜¾ç¤ºå‰3ä¸ªè·¯å¾„çš„èŠ‚ç‚¹ç±»å‹åºåˆ—
                for i, path in enumerate(data[:3]):
                    print(f"    è·¯å¾„{i+1}: {len(path)} ä¸ªèŠ‚ç‚¹")
                    node_types = [node.node_type.value if hasattr(node, 'node_type') else type(node).__name__ for node in path]
                    print(f"      èŠ‚ç‚¹ç±»å‹åºåˆ—: {node_types}")
        
        # å°è¯•è·å–question_idï¼ˆä»æ–‡ä»¶åï¼‰
        import re
        match = re.search(r'(\d+)\.pkl', file_path)
        question_id = match.group(1) if match else "unknown"
        if match:
            print(f"  Question ID: {question_id}")
            
        # æ˜¾ç¤ºæ–‡ä»¶ä¸­çš„SQLæŸ¥è¯¢ï¼ˆå¦‚æœæœ‰ï¼‰
        if isinstance(data, list) and len(data) > 0:
            print(f"  åŒ…å«çš„SQLæŸ¥è¯¢ç¤ºä¾‹:")
            sql_count = 0
            for i, path in enumerate(data):
                if len(path) > 0 and hasattr(path[-1], 'final_sql_query') and path[-1].final_sql_query:
                    sql = path[-1].final_sql_query
                    print(f"    è·¯å¾„{i+1} SQL: {sql}")
                    sql_count += 1
                    if sql_count >= 4:  # åªæ˜¾ç¤ºå‰4ä¸ªSQL
                        break
        
        # åªæ‰“å°ç¬¬ä¸€ä¸ªè·¯å¾„çš„æ ‘ç»“æ„ï¼ˆé¿å…åºåˆ—åŒ–é—®é¢˜ï¼‰
        if isinstance(data, list) and len(data) > 0 and isinstance(data[0], list):
            first_path = data[0]
            print_simple_tree_structure(first_path, show_details=show_node_details)
                
    except Exception as e:
        print(f"  âŒ è¯»å–é”™è¯¯: {e}")
        import traceback
        print(f"  è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

def analyze_and_save_to_file(file_path, output_file, show_node_details=True):
    """
    åˆ†ææ–‡ä»¶å¹¶ä¿å­˜åˆ°æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼ˆé™é»˜æ¨¡å¼ï¼Œä¸è¾“å‡ºåˆ°æ§åˆ¶å°ï¼‰
    """
    # ä¿å­˜åŸå§‹æ ‡å‡†è¾“å‡º
    original_stdout = sys.stdout
    
    try:
        # è®¾ç½®è¾“å‡ºåˆ°æ–‡ä»¶
        setup_logging(output_file)
        
        # å†™å…¥åˆ†æå¼€å§‹ä¿¡æ¯
        print(f"å¼€å§‹åˆ†ææ–‡ä»¶: {file_path}")
        print(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        
        # åˆ†ææ–‡ä»¶ï¼ˆæ‰€æœ‰è¾“å‡ºç›´æ¥åˆ°æ–‡ä»¶ï¼‰
        view_one_file(file_path, show_node_details=show_node_details)
        
        # å†™å…¥åˆ†æç»“æŸä¿¡æ¯
        print("\n" + "=" * 60)
        print("åˆ†æå®Œæˆ!")
        
    finally:
        # æ¢å¤æ ‡å‡†è¾“å‡º
        sys.stdout = original_stdout

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import os
    from pathlib import Path
    
    # é…ç½®è·¯å¾„
    input_dir = "results/Qwen2.5-Coder-7B-Instruct/kramabench/dev_main"
    output_dir = "logs/dev_main_analysis"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # æŸ¥æ‰¾æ‰€æœ‰pklæ–‡ä»¶
    pkl_files = list(Path(input_dir).glob("*.pkl"))
    
    if not pkl_files:
        print(f"åœ¨ç›®å½• {input_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°pklæ–‡ä»¶")
        exit(1)
    
    print(f"æ‰¾åˆ° {len(pkl_files)} ä¸ªpklæ–‡ä»¶ï¼Œå¼€å§‹åˆ†æ...")
    
    # éå†æ¯ä¸ªpklæ–‡ä»¶
    for pkl_file in pkl_files:
        # è·å–æ–‡ä»¶åï¼ˆä¸å¸¦æ‰©å±•åï¼‰
        file_stem = pkl_file.stem
        
        # æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file = Path(output_dir) / f"{file_stem}.log"
        
        # åˆ†æå¹¶ä¿å­˜åˆ°æ–‡ä»¶
        analyze_and_save_to_file(str(pkl_file), str(output_file), show_node_details=True)
        
        print(f"âœ… {file_stem}.pkl -> {file_stem}.log")
    
    print(f"\nğŸ‰ æ‰€æœ‰æ–‡ä»¶åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ“Š å…±å¤„ç†äº† {len(pkl_files)} ä¸ªæ–‡ä»¶")
    
    